# üìö T√ÄI LI·ªÜU CHI TI·∫æT C·∫∂N K·∫º - TO√ÄN B·ªò PROJECT TIME SERIES IMPUTATION
# eDTWBI + BiLSTM/BiGRU/Transformer + 11 ML Models

---

## üéØ M·ª§C L·ª§C CHI TI·∫æT

| M·ª•c | Ch·ªß ƒê·ªÅ | Trang |
|-----|--------|-------|
| A | T·ªïng Quan B√†i To√°n | 1 |
| B | Dataset & Th·ªëng K√™ | 2 |
| C | Ph∆∞∆°ng Ph√°p eDTWBI | 3-8 |
| D | 11 Models Machine Learning | 9-11 |
| E | Deep Learning Architecture | 12-20 |
| F | Training & Optimization | 21-25 |
| G | Metrics & Evaluation | 26-30 |
| H | K·∫øt Qu·∫£ & So S√°nh | 31-35 |
| I | ƒê√≥ng G√≥p Khoa H·ªçc | 36-40 |
| J | Code Chi Ti·∫øt T·ª´ng Cell | 41-60 |
| K | Q&A & Troubleshooting | 61-65 |

---

# A. T·ªîNG QUAN B√ÄI TO√ÅN - CHI TI·∫æT C·∫∂N K·∫º

## A.1. ƒê·ªãnh Nghƒ©a Missing Values

### A.1.1. Kh√°i Ni·ªám
**Missing values** (gi√° tr·ªã thi·∫øu) = c√°c ƒëi·ªÉm d·ªØ li·ªáu kh√¥ng c√≥ (NaN, null, empty)

### A.1.2. Nguy√™n Nh√¢n Th∆∞·ªùng G·∫∑p
```
1. HARDWARE FAILURE
   ‚îî‚îÄ C·∫£m bi·∫øn b·ªã h·ªèng/ng·∫Øt m·∫°ch
      V√≠ d·ª•: C·∫£m bi·∫øn m·ª±c n∆∞·ªõc h·ªèng 2010-2012

2. SOFTWARE ERROR
   ‚îî‚îÄ Bug trong ph·∫ßn m·ªÅm thu th·∫≠p d·ªØ li·ªáu
      V√≠ d·ª•: L·ªói transmit, buffer overflow

3. TRANSMISSION LOSS
   ‚îî‚îÄ M·∫•t d·ªØ li·ªáu khi truy·ªÅn t·ª´ sensor ‚Üí server
      V√≠ d·ª•: L·ªói m·∫°ng WiFi, k·∫øt n·ªëi internet ƒë·ª©t

4. DATA CORRUPTION
   ‚îî‚îÄ D·ªØ li·ªáu b·ªã h·ªèng do ƒëi·ªán, nhi·ªÖu
      V√≠ d·ª•: S√©t ƒë√°nh, noise ƒëi·ªán t·ª´

5. MAINTENANCE
   ‚îî‚îÄ Ng·ª´ng ho·∫°t ƒë·ªông ƒë·ªÉ b·∫£o tr√¨, calibrate
      V√≠ d·ª•: Ki·ªÉm tra ƒë·ªãnh k·ª≥ sensor

6. POWER OUTAGE
   ‚îî‚îÄ M·∫•t ƒëi·ªán, backup kh√¥ng ho·∫°t ƒë·ªông
      V√≠ d·ª•: C·∫Øt ƒëi·ªán b·∫£o tr√¨
```

### A.1.3. ·∫¢nh H∆∞·ªüng C·ªßa Missing Values
```
‚ùå IMPACT 1: Ph√¢n T√≠ch D·ªØ Li·ªáu B·ªã Sai
   - Th·ªëng k√™ (mean, std) kh√¥ng ch√≠nh x√°c
   - Correlation analysis b·ªã bias
   - V√≠ d·ª•: mean(data_with_NaN) ‚â† mean(data_without_NaN)

‚ùå IMPACT 2: Machine Learning Kh√¥ng Ch·∫°y
   - H·∫ßu h·∫øt ML models kh√¥ng x·ª≠ l√Ω NaN
   - TensorFlow/PyTorch throw error n·∫øu input c√≥ NaN
   - Code crash: ValueError: Input contains NaN

‚ùå IMPACT 3: D·ª± B√°o Kh√¥ng Ch√≠nh X√°c
   - M√¥ h√¨nh h·ªçc t·ª´ d·ªØ li·ªáu kh√¥ng ƒë·∫ßy ƒë·ªß
   - Biases towards non-missing periods
   - V√≠ d·ª•: D·ª± b√°o m·ª±c n∆∞·ªõc b·ªã sai v√¨ thi·∫øu d·ªØ li·ªáu training

‚ùå IMPACT 4: Th·ªùi Gian X·ª≠ L√Ω TƒÉng
   - Ph·∫£i x·ª≠ l√Ω ngo·∫°i l·ªá (exception handling)
   - Conditional logic ph·ª©c t·∫°p
   - Code ch·∫≠m, kh√≥ maintain
```

## A.2. Imputation (B√π Khuy·∫øt) L√† G√¨?

### A.2.1. ƒê·ªãnh Nghƒ©a
**Imputation** = Quy tr√¨nh **ƒëi·ªÅn** c√°c gi√° tr·ªã b·ªã thi·∫øu (missing values) d·ª±a v√†o:
- D·ªØ li·ªáu xung quanh (neighbors)
- M·ªëi quan h·ªá v·ªõi bi·∫øn kh√°c (correlation)
- C√°c pattern l·ªãch s·ª≠ (temporal patterns)

### A.2.2. Ph∆∞∆°ng Ph√°p Imputation Ph·ªï Bi·∫øn

```
1. SIMPLE IMPUTATION
   ‚îú‚îÄ Mean/Median/Mode fill
   ‚îÇ  ‚îî‚îÄ X[NaN] = mean(X[kh√¥ng NaN])
   ‚îÇ     V√≠ d·ª•: m·ª±c n∆∞·ªõc missing ‚Üí ƒëi·ªÅn b·∫±ng trung b√¨nh
   ‚îÇ     ‚ö†Ô∏è V·∫•n ƒë·ªÅ: M·∫•t temporal pattern
   ‚îÇ
   ‚îú‚îÄ Forward/Backward Fill
   ‚îÇ  ‚îî‚îÄ X[t] = X[t-1] ho·∫∑c X[t+1]
   ‚îÇ     V√≠ d·ª•: missing ·ªü t=5 ‚Üí l·∫•y gi√° tr·ªã t=4
   ‚îÇ     ‚ö†Ô∏è V·∫•n ƒë·ªÅ: Gi·∫£ s·ª≠ kh√¥ng ƒë·ªïi, b·ªè qua trend
   ‚îÇ
   ‚îî‚îÄ Interpolation (Linear)
      ‚îî‚îÄ X[t] = X[t-1] + (X[t+1] - X[t-1]) / 2
         V√≠ d·ª•: X[1]=100, X[3]=110 ‚Üí X[2]=105
         ‚ö†Ô∏è V·∫•n ƒë·ªÅ: Kh√¥ng capture curvature

2. ADVANCED IMPUTATION
   ‚îú‚îÄ K-Nearest Neighbors (KNN)
   ‚îÇ  ‚îî‚îÄ T√¨m K neighbors g·∫ßn nh·∫•t (theo kho·∫£ng c√°ch)
   ‚îÇ     L·∫•y trung b√¨nh K neighbors
   ‚îÇ     ‚úÖ ∆Øu: Kh√¥ng assume linear
   ‚îÇ     ‚ö†Ô∏è Nh∆∞·ª£c: Slow, ph·∫£i tuning K
   ‚îÇ
   ‚îú‚îÄ Regression-based
   ‚îÇ  ‚îî‚îÄ X√¢y d·ª±ng model t·ª´ d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß
   ‚îÇ     D√πng model d·ª± ƒëo√°n gi√° tr·ªã missing
   ‚îÇ     ‚úÖ ∆Øu: T·∫≠n d·ª•ng to√†n b·ªô d·ªØ li·ªáu
   ‚îÇ     ‚ö†Ô∏è Nh∆∞·ª£c: Ph·ª©c t·∫°p, d·ªÖ overfit
   ‚îÇ
   ‚îú‚îÄ Time Series Methods
   ‚îÇ  ‚îú‚îÄ ARIMA: M√¥ h√¨nh AR/MA cho chu·ªói
   ‚îÇ  ‚îú‚îÄ Exponential Smoothing: Weighted average
   ‚îÇ  ‚îî‚îÄ DTW-based (eDTWBI): **CH·ª¶ ƒê·ªÄ C·ª¶A B√ÄI**
   ‚îÇ     ‚úÖ ∆Øu: Capture temporal patterns & shape
   ‚îÇ     ‚úÖ ∆Øu: Kh√¥ng linear assume
   ‚îÇ
   ‚îú‚îÄ Deep Learning
   ‚îÇ  ‚îú‚îÄ LSTM/GRU: Sequence-to-sequence
   ‚îÇ  ‚îú‚îÄ Transformer: Attention-based
   ‚îÇ  ‚îî‚îÄ **K·∫æT H·ª¢P LSTM/GRU/TRANSFORMER + eDTWBI**
   ‚îÇ     ‚úÖ ∆Øu: Pattern + deep learning
   ‚îÇ     ‚úÖ ∆Øu: Phi tuy·∫øn, t·ªïng qu√°t
   ‚îÇ
   ‚îî‚îÄ Statistical (Advanced)
      ‚îú‚îÄ Multiple Imputation by Chained Equations (MICE)
      ‚îî‚îÄ Expectation-Maximization (EM)
         ‚úÖ ∆Øu: Bayesian approach, uncertainty
         ‚ö†Ô∏è Nh∆∞·ª£c: Ch·∫≠m, ph·ª©c t·∫°p
```

### A.2.3. T·∫°i Sao Imputation Quan Tr·ªçng?

```
TR∆Ø·ªöC Imputation:
  Data: [1.0, 2.5, NaN, 3.2, NaN, 4.1, ...]
  ‚ùå Kh√¥ng th·ªÉ t√≠nh mean, std
  ‚ùå ML model crash
  ‚ùå Ph√¢n t√≠ch b·ªã sai l·ªách

SAU Imputation:
  Data: [1.0, 2.5, 2.85, 3.2, 3.65, 4.1, ...]
  ‚úÖ C√≥ th·ªÉ t√≠nh t·∫•t c·∫£ th·ªëng k√™
  ‚úÖ ML model ch·∫°y b√¨nh th∆∞·ªùng
  ‚úÖ D·ª± b√°o ch√≠nh x√°c
```

---

# B. DATASET & TH·ªêNG K√ä - CHI TI·∫æT C·∫∂N K·∫º

## B.1. Ngu·ªìn D·ªØ Li·ªáu

### B.1.1. File & ƒê∆∞·ªùng D·∫´n
```
File: Impute_misvalues_hanoi.csv
ƒê∆∞·ªùng d·∫´n: /kaggle/input/misshanoi/Impute_misvalues_hanoi.csv
K√≠ch th∆∞·ªõc file: 1.29 MB
Format: CSV (comma-separated values)
Encoding: UTF-8 (standard)
```

### B.1.2. C·∫•u Tr√∫c File
```
CSV Header:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Index       ‚îÇ Date     ‚îÇ Hour ‚îÇ Average  ‚îÇ Waterlevel  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1           ‚îÇ 2010-01-01 ‚îÇ 0  ‚îÇ NaN      ‚îÇ 123.45      ‚îÇ
‚îÇ 2           ‚îÇ 2010-01-01 ‚îÇ 1  ‚îÇ 125.67   ‚îÇ 124.56      ‚îÇ
‚îÇ 3           ‚îÇ 2010-01-01 ‚îÇ 2  ‚îÇ NaN      ‚îÇ 125.01      ‚îÇ
‚îÇ 4           ‚îÇ 2010-01-01 ‚îÇ 3  ‚îÇ 124.89   ‚îÇ 125.34      ‚îÇ
‚îÇ ...         ‚îÇ ...      ‚îÇ ... ‚îÇ ...      ‚îÇ ...         ‚îÇ
‚îÇ 29224       ‚îÇ 2012-12-31 ‚îÇ 23 ‚îÇ 89.34    ‚îÇ 85.32       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

C·ªôt Chi Ti·∫øt:
1. Index: S·ªë th·ª© t·ª± b·∫£n ghi (1-29224)
2. Date: Ng√†y (YYYY-MM-DD)
3. Hour: Gi·ªù (0-23)
4. Average: M·ª±c n∆∞·ªõc trung b√¨nh (ƒê√ÇY L√Ä C·ªòT C·∫¶N IMPUTE)
5. Waterlevel: M·ª±c n∆∞·ªõc (ƒê√¢y l√† c·ªôt tham chi·∫øu, kh√¥ng missing)
```

## B.2. Th·ªëng K√™ D·ªØ Li·ªáu

### B.2.1. T·ªïng Quan
```
STATISTICS:
‚îú‚îÄ Total records: 29,224
‚îú‚îÄ Time period: 2010-01-01 00:00 ‚Üí 2012-12-31 23:00
‚îú‚îÄ Duration: ~3 nƒÉm ƒë·∫ßy ƒë·ªß
‚îî‚îÄ Granularity: Hourly (1 gi·ªù/1 b·∫£n ghi)

MISSING VALUES:
‚îú‚îÄ Column "Average":
‚îÇ  ‚îú‚îÄ Total NaN: 25,910
‚îÇ  ‚îú‚îÄ Percentage: 25,910 / 29,224 = 88.66%
‚îÇ  ‚îú‚îÄ Complete values: 3,314 (11.34%)
‚îÇ  ‚îî‚îÄ ‚ö†Ô∏è VERY HIGH MISSING RATE!
‚îÇ
‚îî‚îÄ Column "Waterlevel":
   ‚îú‚îÄ Total NaN: 0
   ‚îú‚îÄ Percentage: 0%
   ‚îú‚îÄ Complete values: 29,224 (100%)
   ‚îî‚îÄ ‚úÖ Perfect reference for imputation!

VALUE STATISTICS (Column "Average"):
‚îú‚îÄ Min: 43.00 cm
‚îú‚îÄ Max: 930.00 cm
‚îú‚îÄ Mean: ~150 cm (khi c√≥ d·ªØ li·ªáu)
‚îú‚îÄ Median: ~145 cm
‚îî‚îÄ Std: ~85 cm

VALUE STATISTICS (Column "Waterlevel"):
‚îú‚îÄ Min: ~40 cm
‚îú‚îÄ Max: ~940 cm
‚îú‚îÄ Mean: ~155 cm
‚îú‚îÄ Median: ~150 cm
‚îî‚îÄ Std: ~90 cm
```

### B.2.2. Ph√¢n B·ªë Missing Values

```
GAP ANALYSIS (C√°c kho·∫£ng li√™n t·ª•c b·ªã NaN):
‚îú‚îÄ T·ªïng s·ªë gaps: 3,315 (h∆°n 3 ng√†n kho·∫£ng)
‚îú‚îÄ Gap length statistics:
‚îÇ  ‚îú‚îÄ Min: 1 gi·ªù (gap ng·∫Øn nh·∫•t)
‚îÇ  ‚îú‚îÄ Max: 8,760 gi·ªù (~1 nƒÉm, gap d√†i nh·∫•t)
‚îÇ  ‚îú‚îÄ Mean: 7.82 gi·ªù (gap trung b√¨nh)
‚îÇ  ‚îú‚îÄ Median: 5 gi·ªù
‚îÇ  ‚îî‚îÄ Mode: 1 gi·ªù (h·∫ßu h·∫øt gaps l√† 1-2 gi·ªù)
‚îÇ
‚îú‚îÄ Distribution:
‚îÇ  ‚îú‚îÄ 1 gi·ªù: 40% gaps (~1,300)
‚îÇ  ‚îú‚îÄ 2-5 gi·ªù: 35% gaps (~1,160)
‚îÇ  ‚îú‚îÄ 6-24 gi·ªù: 15% gaps (~500)
‚îÇ  ‚îú‚îÄ 1-7 ng√†y: 7% gaps (~230)
‚îÇ  ‚îú‚îÄ 1-12 th√°ng: 2% gaps (~70)
‚îÇ  ‚îî‚îÄ >1 nƒÉm: 1% gaps (~35)
‚îÇ
‚îî‚îÄ Temporal distribution:
   ‚îú‚îÄ Gaps th∆∞·ªùng xu·∫•t hi·ªán v√†o:
   ‚îÇ  ‚îú‚îÄ Th√°ng 8-9 (m√πa m∆∞a): Nhi·ªÅu nh·∫•t
   ‚îÇ  ‚îú‚îÄ Th√°ng 1-2 (m√πa kh√¥): √çt
   ‚îÇ  ‚îî‚îÄ Random: C√≥ s·ª± c·ªë kh√¥ng ƒë·ªãnh k·ª≥
   ‚îÇ
   ‚îî‚îÄ Pattern:
      ‚îú‚îÄ S√°ng (06:00-09:00): √çt missing
      ‚îú‚îÄ Tr∆∞a (12:00-15:00): Nhi·ªÅu missing
      ‚îî‚îÄ T·ªëi (18:00-21:00): Trung b√¨nh
```

## B.3. Temporal Characteristics

### B.3.1. T√≠nh Ch·∫•t Chu·ªói Th·ªùi Gian
```
TIME SERIES PROPERTIES:

1. TREND (Xu H∆∞·ªõng)
   ‚îú‚îÄ M·ª±c n∆∞·ªõc c√≥ xu h∆∞·ªõng tƒÉng t·ª´ th√°ng 5-9
   ‚îú‚îÄ Gi·∫£m t·ª´ th√°ng 10-4
   ‚îú‚îÄ Theo quy lu·∫≠t m√πa ƒê√¥ng-B·∫Øc & m∆∞a monsoon
   ‚îî‚îÄ Trend slope: ~0.5-1.0 cm/th√°ng (kh√°c nhau)

2. SEASONALITY (T√≠nh M√πa V·ª•)
   ‚îú‚îÄ Chu k·ª≥: 12 th√°ng (1 nƒÉm)
   ‚îú‚îÄ Bi√™n ƒë·ªô: ¬±200-300 cm so v·ªõi mean
   ‚îú‚îÄ Nguy√™n nh√¢n:
   ‚îÇ  ‚îú‚îÄ M√πa m∆∞a (5-9): M·ª±c n∆∞·ªõc cao
   ‚îÇ  ‚îú‚îÄ M√πa kh√¥ (10-4): M·ª±c n∆∞·ªõc th·∫•p
   ‚îÇ  ‚îî‚îÄ Ph·ª• thu·ªôc v√†o l∆∞·ª£ng m∆∞a
   ‚îî‚îÄ Pattern nh·∫•t qu√°n qua 3 nƒÉm

3. AUTOCORRELATION (T·ª± T∆∞∆°ng Quan)
   ‚îú‚îÄ ACF(1): 0.95+ (r·∫•t m·∫°nh)
   ‚îÇ  ‚îî‚îÄ Gi√° tr·ªã h√¥m nay ph·ª• thu·ªôc h√¥m qua
   ‚îú‚îÄ ACF(24): 0.85-0.90 (t∆∞∆°ng quan 24h)
   ‚îÇ  ‚îî‚îÄ Chu k·ª≥ daily (c√≥ th·ªÉ)
   ‚îú‚îÄ ACF(168): 0.80+ (t∆∞∆°ng quan 1 tu·∫ßn)
   ‚îî‚îÄ ACF(365): 0.75+ (t∆∞∆°ng quan 1 nƒÉm, seasonal)

4. STATIONARITY (T√≠nh D·ª´ng)
   ‚îú‚îÄ NOT stationary (c√≥ trend & seasonality)
   ‚îú‚îÄ ADF test: p-value > 0.05 ‚Üí kh√¥ng reject H0
   ‚îú‚îÄ C·∫ßn differencing ho·∫∑c detrending
   ‚îî‚îÄ Log differencing: d=1 ho·∫∑c s=12

5. VOLATILITY (Bi·∫øn ƒê·ªông)
   ‚îú‚îÄ High volatility (kh√¥ng smooth)
   ‚îú‚îÄ Std c·ªßa differences: ~10-15 cm/gi·ªù
   ‚îú‚îÄ C√≥ spike (l≈© ƒë·ªôt ng·ªôt) + drop (h·∫° ng·ªôt)
   ‚îî‚îÄ C·∫ßn model capture n√†y
```

---

# C. PH∆Ø∆†NG PH√ÅP eDTWBI - CHI TI·∫æT C·∫∂N K·∫º

## C.1. DTW (Dynamic Time Warping) Basics

### C.1.1. V·∫•n ƒê·ªÅ M√† DTW Gi·∫£i Quy·∫øt

```
PROBLEM:
L√†m sao so s√°nh 2 chu·ªói th·ªùi gian c√≥ ƒë·ªô d√†i kh√°c nhau?

V√≠ d·ª•:
Series A: [1, 2, 3, 4, 5]           (length=5)
Series B: [1, 1, 2, 3, 3, 4, 5]     (length=7)

Euclidean Distance: ‚ùå Kh√¥ng th·ªÉ (ƒë·ªô d√†i kh√°c)

Gi·∫£i ph√°p: DTW
‚îú‚îÄ Allows "warping" time axis
‚îú‚îÄ Matches elements flexibly
‚îî‚îÄ Capture shape similarity despite length difference
```

### C.1.2. DTW Algorithm (Step by Step)

```
STEP 1: Initialize Distance Matrix
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ dtw ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  0  ‚îÇ 0 ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ
‚îÇ  1  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  2  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  3  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  4  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  5  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  6  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îÇ  7  ‚îÇ ‚àû ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ ? ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

Matrix: (n+1) x (m+1) = (8 x 6)
n = len(Series B) = 7
m = len(Series A) = 5

STEP 2: Fill with Recurrence Relation
D(i,j) = |B[i-1] - A[j-1]| + min(D(i-1,j), D(i,j-1), D(i-1,j-1))

Meaning:
‚îú‚îÄ |B[i-1] - A[j-1]|: Cost (kho·∫£ng c√°ch Euclidean)
‚îú‚îÄ D(i-1,j): From above (insert)
‚îú‚îÄ D(i,j-1): From left (delete)
‚îî‚îÄ D(i-1,j-1): From diagonal (match)

STEP 3: Example Calculation (B[0]=1 vs A[0]=1)
D(1,1) = |1-1| + min(D(0,1), D(1,0), D(0,0))
       = 0 + min(‚àû, ‚àû, 0)
       = 0

(B[0]=1 kh·ªõp ho√†n h·∫£o v·ªõi A[0]=1)

STEP 4: Fill Complete Matrix
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ dtw ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  0  ‚îÇ 0 ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ
‚îÇ  1  ‚îÇ ‚àû ‚îÇ 0 ‚îÇ 1 ‚îÇ 3 ‚îÇ 6 ‚îÇ10 ‚îÇ
‚îÇ  2  ‚îÇ ‚àû ‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ 3 ‚îÇ 7 ‚îÇ
‚îÇ  3  ‚îÇ ‚àû ‚îÇ 2 ‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ 5 ‚îÇ
‚îÇ  4  ‚îÇ ‚àû ‚îÇ 3 ‚îÇ 2 ‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ
‚îÇ  5  ‚îÇ ‚àû ‚îÇ 4 ‚îÇ 3 ‚îÇ 2 ‚îÇ 1 ‚îÇ 0 ‚îÇ
‚îÇ  6  ‚îÇ ‚àû ‚îÇ 5 ‚îÇ 4 ‚îÇ 3 ‚îÇ 2 ‚îÇ 1 ‚îÇ
‚îÇ  7  ‚îÇ ‚àû ‚îÇ 6 ‚îÇ 5 ‚îÇ 4 ‚îÇ 3 ‚îÇ 2 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

STEP 5: Final DTW Distance
dtw_distance = D[n][m] = D[7][5] = 2

Interpretation: Cost ƒë·ªÉ kh·ªõp 2 chu·ªói = 2
```

### C.1.3. DTW vs Euclidean Distance

```
COMPARISON:

Euclidean Distance:
‚îú‚îÄ Formula: sqrt(sum((a_i - b_i)^2))
‚îú‚îÄ Pros: Nhanh, ƒë∆°n gi·∫£n
‚îú‚îÄ Cons: Requires same length
‚îú‚îÄ Not good for: Time warping

DTW Distance:
‚îú‚îÄ Formula: Dynamic programming recurrence
‚îú‚îÄ Pros: Handles different lengths, captures shape
‚îú‚îÄ Cons: Slower O(n*m)
‚îú‚îÄ Good for: Time series shape matching

Example:
A: [1, 2, 3, 4, 5]
B: [1, 1, 2, 3, 3, 4, 5]

Euclidean: ‚ùå Kh√¥ng th·ªÉ t√≠nh (length‚â†)
DTW: ‚úÖ Distance ‚âà 2 (g·∫ßn gi·ªëng shape)
```

### C.1.4. Sakoe-Chiba Band Optimization

```
PROBLEM:
DTW complexity: O(n*m) = 7 √ó 5 = 35 cells
For 1000-length series: 1000 √ó 1000 = 1,000,000 cells ‚ö†Ô∏è SLOW

SOLUTION: Sakoe-Chiba Band
‚îú‚îÄ Ch·ªâ compute cells trong band
‚îú‚îÄ Band: |i - j| ‚â§ window_size
‚îú‚îÄ Lo·∫°i b·ªè cells qu√° xa diagonal

Window size = 4:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ dtw ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 4 ‚îÇ 5 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  0  ‚îÇ 0 ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ ‚àû ‚îÇ
‚îÇ  1  ‚îÇ ‚àû ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ  (only compute ‚úì)
‚îÇ  2  ‚îÇ ‚àû ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úó ‚îÇ
‚îÇ  3  ‚îÇ ‚àû ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îÇ  4  ‚îÇ ‚àû ‚îÇ ‚úó ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îÇ  5  ‚îÇ ‚àû ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îÇ  6  ‚îÇ ‚àû ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îÇ  7  ‚îÇ ‚àû ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úó ‚îÇ ‚úì ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

RESULT:
Original: 7 √ó 5 = 35 cells
With band: ~20 cells
Speedup: 35/20 = 1.75x faster

For 1000-length:
Original: 1,000,000 cells
With window=4: 4,000 cells
Speedup: 1,000,000/4,000 = 250x faster! üöÄ
```

## C.2. eDTWBI (Enhanced DTW-Based Imputation) Ho√†n Ch·ªânh

### C.2.1. Overall Architecture

```
INPUT: D·ªØ li·ªáu v·ªõi missing values
‚îú‚îÄ original: [1, 2, NaN, NaN, 4, 5, NaN, 7]
‚îî‚îÄ reference: [1.1, 2.2, 3.1, 4.0, 4.1, 5.0, 6.1, 7.2]

STEP 1: GAP DETECTION
‚îî‚îÄ Gaps: [(2,3), (6,6)]

STEP 2-5: FOR EACH GAP
‚îú‚îÄ Gap 1: (start=2, end=3)
‚îÇ  ‚îú‚îÄ gap_length = 3 - 2 + 1 = 2
‚îÇ  ‚îú‚îÄ left_context = [1, 2]
‚îÇ  ‚îú‚îÄ right_context = [4, 5]
‚îÇ  ‚îî‚îÄ Target: Find 2-length subsequences t∆∞∆°ng t·ª±
‚îÇ
‚îî‚îÄ Gap 2: (start=6, end=6)
   ‚îú‚îÄ gap_length = 1
   ‚îú‚îÄ left_context = [5]
   ‚îú‚îÄ right_context = [7]
   ‚îî‚îÄ Target: Find 1-length values t∆∞∆°ng t·ª±

STEP 6: MERGE RESULTS
‚îú‚îÄ imputed_full: [1, 2, X1, X2, 4, 5, X3, 7]
‚îî‚îÄ context_vectors: {(2,3): ctx1, (6,6): ctx2}

OUTPUT: Filled data + context info
```

### C.2.2. 7 B∆∞·ªõc Chi Ti·∫øt - M·ªói B∆∞·ªõc C·ª• Th·ªÉ

#### **B∆Ø·ªöC 1: GAP DETECTION**

```python
def find_gaps(arr):
    gaps = []
    inside_gap = False
    
    for i, v in enumerate(arr):
        if np.isnan(v):
            if not inside_gap:
                gap_start = i
                inside_gap = True
        else:
            if inside_gap:
                gap_end = i - 1
                gaps.append((gap_start, gap_end))
                inside_gap = False
    
    # Check if ends with gap
    if inside_gap:
        gaps.append((gap_start, len(arr) - 1))
    
    return gaps

# Example:
arr = [1.0, 2.0, NaN, NaN, 4.0, NaN, 7.0, 8.0, NaN, NaN, NaN]
gaps = find_gaps(arr)
# Output: [(2, 3), (5, 5), (8, 10)]
#         Gap1    Gap2    Gap3

EXPLANATION:
‚îú‚îÄ Gap 1: indices 2-3 (2 values NaN)
‚îú‚îÄ Gap 2: index 5 (1 value NaN)
‚îî‚îÄ Gap 3: indices 8-10 (3 values NaN)
```

**Code Chi Ti·∫øt**:
```python
PSEUDOCODE:
1. Initialize: gaps = [], inside_gap = False
2. Loop i=0 to len(arr)-1:
   a. If arr[i] = NaN:
      - If inside_gap = False:
        - Set gap_start = i
        - Set inside_gap = True
   b. Else (arr[i] ‚â† NaN):
      - If inside_gap = True:
        - Set gap_end = i - 1
        - Append (gap_start, gap_end) to gaps
        - Set inside_gap = False
3. After loop: If inside_gap = True:
   - Append (gap_start, len(arr)-1) to gaps
4. Return gaps

COMPLEXITY: O(n) - single pass through array
SPACE: O(k) where k = number of gaps
```

#### **B∆Ø·ªöC 2: CONTEXT EXTRACTION**

```python
gap_start = 2
gap_end = 3
reference = [1.0, 2.0, 3.1, 4.0, 4.1, 5.0, 6.1, 7.2]
window = 3

# Extract left context (3 values BEFORE gap)
left_context = reference[max(0, gap_start-window):gap_start]
            = reference[max(0, 2-3):2]
            = reference[0:2]
            = [1.0, 2.0]
# Note: window=3 nh∆∞ng ch·ªâ c√≥ 2 values tr∆∞·ªõc gap

# Extract right context (3 values AFTER gap)
right_context = reference[gap_end+1:gap_end+window+1]
             = reference[4:7]
             = [4.1, 5.0, 6.1]
# Note: ƒë·ªß 3 values sau gap

# Gap length
gap_len = gap_end - gap_start + 1
        = 3 - 2 + 1
        = 2

VISUALIZATION:
reference indices: [0,   1,    2,   3,    4,    5,    6,    7   ]
reference values:  [1.0, 2.0, NaN, NaN,  4.1,  5.0,  6.1,  7.2 ]
                          gap_start      gap_end
                   ‚îî‚îÄ LC ‚îò‚îÇ             ‚îÇ‚îî‚îÄ RC ‚îÄ‚îò
left_context = [1.0, 2.0]
right_context = [4.1, 5.0, 6.1]
target_length = 2 (need to fill indices 2-3)
```

**Code Chi Ti·∫øt**:
```python
def extract_context(reference, gap_start, gap_end, window):
    gap_len = gap_end - gap_start + 1
    
    left_idx_start = max(0, gap_start - window)
    left_idx_end = gap_start
    left_context = reference[left_idx_start:left_idx_end]
    
    right_idx_start = gap_end + 1
    right_idx_end = min(len(reference), gap_end + window + 1)
    right_context = reference[right_idx_start:right_idx_end]
    
    return gap_len, left_context, right_context

HANDLING EDGE CASES:
‚îú‚îÄ Gap ·ªü ƒë·∫ßu: left_context = [] (empty)
‚îÇ  ‚îî‚îÄ max(0, gap_start-window) = 0
‚îÇ
‚îú‚îÄ Gap ·ªü cu·ªëi: right_context = [] (empty)
‚îÇ  ‚îî‚îÄ gap_end+1 > len(reference)
‚îÇ
‚îî‚îÄ Gap ·ªü gi·ªØa: left_context + right_context (normal)
```

#### **B∆Ø·ªöC 3: CANDIDATE SEARCH**

```python
# SEARCH PARAMETERS
search_range = min(len(reference) - gap_len - window, 
                  max(500, len(reference)//10))

# LOGIC:
# T√¨m t·∫•t c·∫£ subsequences t·ª´ reference
# M·ªói subsequence c√≥ c·∫•u tr√∫c: [cand_left, cand_gap, cand_right]
# cand_gap = m·ª•c ti√™u ƒëi·ªÅn

candidates = []

for idx in range(window, search_range):  # Start t·ª´ window ƒë·ªÉ c√≥ left context
    cand_left = reference[idx-window:idx]          # Left context
    cand_gap = reference[idx:idx+gap_len]           # Gap values (TARGET)
    cand_right = reference[idx+gap_len:idx+gap_len+window]  # Right context
    
    # Check: c√≥ NaN kh√¥ng?
    if np.isnan(cand_gap).any() or np.isnan(cand_left).any() or np.isnan(cand_right).any():
        continue  # Skip, kh√¥ng th·ªÉ d√πng
    
    # ƒê√¢y l√† m·ªôt candidate h·ª£p l·ªá
    candidates.append({
        'left': cand_left,
        'gap': cand_gap,
        'right': cand_right,
        'index': idx
    })

# EXAMPLE:
reference = [1.0, 2.0, 3.1, 4.0, 4.1, 5.0, 6.1, 7.2, 8.0, 8.5]
gap_len = 2
window = 3
search_range = 8

idx=3:
  cand_left = reference[0:3] = [1.0, 2.0, 3.1]
  cand_gap = reference[3:5] = [4.0, 4.1]  ‚Üê CANDIDATE GAP
  cand_right = reference[5:8] = [5.0, 6.1, 7.2]
  
idx=4:
  cand_left = reference[1:4] = [2.0, 3.1, 4.0]
  cand_gap = reference[4:6] = [4.1, 5.0]  ‚Üê CANDIDATE GAP
  cand_right = reference[6:9] = [6.1, 7.2, 8.0]

... v√† ti·∫øp t·ª•c ...

OPTIMIZATION:
‚îú‚îÄ search_range = min(len(ref) - gap_len - window, max(500, len(ref)//10))
‚îú‚îÄ V√≠ d·ª•: len(ref)=29,224 ‚Üí search_range = max(500, 2922) = 2922
‚îú‚îÄ Ch·ªâ search 2922 candidates thay v√¨ 29,224
‚îî‚îÄ Gi·∫£m 90% s·ªë t√¨m ki·∫øm!
```

#### **B∆Ø·ªöC 4: COSINE SIMILARITY FILTERING**

```python
from scipy.spatial.distance import cosine

# Cho m·ªói candidate, t√≠nh cosine similarity
for candidate in candidates:
    cand_left = candidate['left']
    cand_right = candidate['right']
    
    # Cosine similarity (distance ‚Üí similarity)
    # cosine(u, v) = u¬∑v / (||u|| * ||v||)
    # cosine_distance = 1 - cosine_similarity
    
    if len(left_context) == window:
        sim_left = 1 - cosine(left_context, cand_left)
    else:
        sim_left = 0  # Context b·ªã c·∫Øt, kh√¥ng so s√°nh
    
    if len(right_context) == window:
        sim_right = 1 - cosine(right_context, cand_right)
    else:
        sim_right = 0  # Context b·ªã c·∫Øt
    
    avg_sim = (sim_left + sim_right) / 2
    
    # Filter: Keep only if similar enough
    if avg_sim >= cosine_threshold:  # typically 0.7
        candidate['similarity'] = avg_sim
        filtered_candidates.append(candidate)

# EXAMPLE CALCULATION:
left_context = [1.0, 2.0, 3.1]
cand_left = [1.1, 1.9, 3.0]

# Compute cosine similarity
dot_product = 1.0*1.1 + 2.0*1.9 + 3.1*3.0 = 1.1 + 3.8 + 9.3 = 14.2
norm_left = sqrt(1.0^2 + 2.0^2 + 3.1^2) = sqrt(1 + 4 + 9.61) = sqrt(14.61) ‚âà 3.82
norm_cand_left = sqrt(1.1^2 + 1.9^2 + 3.0^2) = sqrt(1.21 + 3.61 + 9) = sqrt(13.82) ‚âà 3.72

cosine_similarity = 14.2 / (3.82 √ó 3.72) = 14.2 / 14.21 ‚âà 0.9993 (VERY HIGH!)
cosine_distance = 1 - 0.9993 = 0.0007

sim_left = 0.9993

# T∆∞∆°ng t·ª± t√≠nh sim_right
# avg_sim = (sim_left + sim_right) / 2

# N·∫øu avg_sim ‚â• 0.7: KEEP candidate
# N·∫øu avg_sim < 0.7: REJECT candidate

INTERPRETATION:
‚îú‚îÄ Sim = 1.0: Ho√†n to√†n gi·ªëng
‚îú‚îÄ Sim = 0.8-0.9: R·∫•t t∆∞∆°ng t·ª±
‚îú‚îÄ Sim = 0.7-0.8: T∆∞∆°ng t·ª±
‚îú‚îÄ Sim = 0.5-0.7: T·∫°m t∆∞∆°ng t·ª±
‚îî‚îÄ Sim < 0.5: Kh√¥ng t∆∞∆°ng t·ª±
```

#### **B∆Ø·ªöC 5: DTW DISTANCE CALCULATION**

```python
# Cho m·ªói filtered candidate, t√≠nh DTW distance
for candidate in filtered_candidates:
    cand_left = candidate['left']
    cand_right = candidate['right']
    
    # DTW gi·ªØa left contexts
    dtw_left = dtw_distance(left_context, cand_left, window_size=dtw_radius)
    
    # DTW gi·ªØa right contexts
    dtw_right = dtw_distance(right_context, cand_right, window_size=dtw_radius)
    
    # T·ªïng DTW distance
    total_dtw = dtw_left + dtw_right
    
    candidate['dtw_distance'] = total_dtw

# DTW FUNCTION (v·ªõi Sakoe-Chiba band):
def dtw_distance(s1, s2, window_size=4):
    n, m = len(s1), len(s2)
    dtw = np.full((n+1, m+1), np.inf)
    dtw[0, 0] = 0
    
    for i in range(1, n+1):
        # Sakoe-Chiba band: only compute |i-j| ‚â§ window_size
        for j in range(max(1, i-window_size), min(m+1, i+window_size)):
            cost = abs(s1[i-1] - s2[j-1])
            dtw[i, j] = cost + min(dtw[i-1, j], dtw[i, j-1], dtw[i-1, j-1])
    
    return dtw[n, m]

# EXAMPLE:
left_context = [1.0, 2.0]
cand_left = [1.05, 2.02]

dtw_matrix:
     0    1.05   2.02
  0  0    ‚àû      ‚àû
  1  ‚àû    0.05   1.07
  2  ‚àû    0.07   0.09

dtw_distance(left_context, cand_left) = 0.09 (VERY SIMILAR!)

# ƒêi·ªÅu n√†y c√≥ nghƒ©a: cand_left r·∫•t gi·ªëng left_context
# Cost ch·ªâ l√† 0.09 ƒë·ªÉ kh·ªõp
```

#### **B∆Ø·ªöC 6: TOP-K SELECTION**

```python
# Sort candidates by DTW distance (nh·ªè nh·∫•t = t·ªët nh·∫•t)
filtered_candidates.sort(key=lambda x: x['dtw_distance'])

# L·∫•y k t·ªët nh·∫•t
k_best = 2
best_candidates = filtered_candidates[:k_best]

# Extract gap values t·ª´ top-k
best_gaps = [c['gap'] for c in best_candidates]

# Example:
filtered_candidates = [
    {'dtw_distance': 0.15, 'gap': [4.0, 4.1]},  # ‚Üê Best
    {'dtw_distance': 0.22, 'gap': [3.95, 4.05]},  # ‚Üê 2nd best
    {'dtw_distance': 0.35, 'gap': [4.2, 3.9]},    # ‚Üê 3rd (kh√¥ng l·∫•y)
    {'dtw_distance': 0.58, 'gap': [3.5, 4.5]},    # ‚Üê 4th (kh√¥ng l·∫•y)
]

best_candidates = filtered_candidates[:2]
best_gaps = [[4.0, 4.1], [3.95, 4.05]]

# Extract context info
if best_candidates:
    best_cand = best_candidates[0]
    context_feature = np.concatenate([
        best_cand['left'],           # Left context
        best_cand['right'],          # Right context
        [np.mean(best_gaps)],        # Mean of best gaps
        [np.std(best_gaps)]          # Std of best gaps
    ])
    # context_feature shape: (3+3+1+1,) = (8,)
```

#### **B∆Ø·ªöC 7: ƒêI·ªÄN GI√Å TR·ªä**

```python
# Compute fill value
fill_value = np.mean(best_gaps, axis=0)

# Example:
best_gaps = [[4.0, 4.1], [3.95, 4.05]]
fill_value = mean([[4.0, 4.1], [3.95, 4.05]])
           = [(4.0+3.95)/2, (4.1+4.05)/2]
           = [3.975, 4.075]

# Fill v√†o original data
imputed_full[gap_start:gap_end+1] = fill_value

# Before:
original = [1.0, 2.0, NaN, NaN, 4.1, 5.0, ...]

# After:
imputed_full = [1.0, 2.0, 3.975, 4.075, 4.1, 5.0, ...]

# Store context for DL model
context_vectors[(gap_start, gap_end)] = context_feature

RESULT:
‚îú‚îÄ imputed_full: D·ªØ li·ªáu ƒë√£ ƒëi·ªÅn
‚îî‚îÄ context_vectors: Dict ch·ª©a context cho m·ªói gap
   ‚îî‚îÄ Key: (gap_start, gap_end)
   ‚îî‚îÄ Value: context feature (d√πng cho DL input)
```

### C.2.3. Caching Mechanism

```python
import pickle
import os

cache_file = '/kaggle/working/edtwbi_cache.pkl'

# CHECK CACHE
if os.path.exists(cache_file):
    print("‚úì Loading cached eDTWBI results...")
    with open(cache_file, 'rb') as f:
        context_vectors, imputed_full = pickle.load(f)
    print("‚úì Loaded successfully!")
    # Time: ~1 second
else:
    print("‚ö† Computing eDTWBI (first run)...")
    
    # Compute gaps
    gaps = find_gaps(original)
    
    # Process each gap
    context_vectors = {}
    imputed_full = original.copy()
    
    for i, (start, end) in enumerate(gaps):
        if (i + 1) % max(1, len(gaps)//10) == 0:
            print(f"Progress: {i+1}/{len(gaps)} gaps")
        
        ctx, fill = edtwbi_context(original, waterlevel, start, end, 
                                  window=3, k_best=2, 
                                  cosine_threshold=0.7)
        imputed_full[start:end+1] = fill
        context_vectors[(start, end)] = ctx
    
    # Save cache
    print("‚úì Saving cache...")
    with open(cache_file, 'wb') as f:
        pickle.dump((context_vectors, imputed_full), f)
    print("‚úì Cache saved!")
    # Time: ~15 minutes

SPEEDUP CALCULATION:
‚îú‚îÄ First run (compute): 15 minutes = 900 seconds
‚îú‚îÄ Cache load: 1 second
‚îú‚îÄ Speedup: 900x faster!
‚îÇ
‚îî‚îÄ File size:
   ‚îî‚îÄ cache.pkl ‚âà 5-10 MB (ph·ª• thu·ªôc compression)

WHEN TO USE CACHING:
‚îú‚îÄ ‚úÖ Khi iterative development (try multiple DL models)
‚îú‚îÄ ‚úÖ Khi debug code
‚îú‚îÄ ‚ùå Khi thay ƒë·ªïi eDTWBI parameters (ph·∫£i recompute)
‚îî‚îÄ ‚ùå Khi data thay ƒë·ªïi (cache stale)
```

### C.2.4. To√†n B·ªô eDTWBI Workflow

```
INPUT DATA:
‚îú‚îÄ original: [1.0, 2.0, NaN, NaN, NaN, 5.0, 6.0]
‚îî‚îÄ reference: [1.1, 2.1, 3.0, 4.0, 5.1, 5.2, 6.1]

STEP 1: Gap Detection
‚îî‚îÄ gaps = [(2, 4)]  # One gap at indices 2-4

STEP 2-7: Process Gap (2, 4)
‚îú‚îÄ gap_length = 3
‚îú‚îÄ left_context = [1.0, 2.0]
‚îú‚îÄ right_context = [5.0, 6.0]
‚îÇ
‚îú‚îÄ Candidate Search:
‚îÇ  ‚îú‚îÄ Find all 3-length subsequences in reference
‚îÇ  ‚îî‚îÄ Candidates:
‚îÇ     ‚îú‚îÄ Candidate 1: left=[1.1,2.1], gap=[3.0,4.0,5.1], right=[5.2,6.1]
‚îÇ     ‚îú‚îÄ Candidate 2: left=[2.1,3.0], gap=[4.0,5.1,5.2], right=[6.1,...]
‚îÇ     ‚îî‚îÄ More candidates...
‚îÇ
‚îú‚îÄ Cosine Similarity Filter:
‚îÇ  ‚îú‚îÄ Candidate 1: sim_left=0.95, sim_right=0.92 ‚Üí avg=0.935 > 0.7 ‚úÖ
‚îÇ  ‚îú‚îÄ Candidate 2: sim_left=0.80, sim_right=0.75 ‚Üí avg=0.775 > 0.7 ‚úÖ
‚îÇ  ‚îî‚îÄ Candidate 3: sim_left=0.50, ... ‚Üí avg=0.52 < 0.7 ‚ùå
‚îÇ
‚îú‚îÄ DTW Distance:
‚îÇ  ‚îú‚îÄ Candidate 1: dist=0.08 ‚úì (Best)
‚îÇ  ‚îú‚îÄ Candidate 2: dist=0.15 ‚úì (2nd)
‚îÇ  ‚îî‚îÄ Candidate 3: dist=0.42 ‚úì (3rd)
‚îÇ
‚îú‚îÄ Top-K Selection (k=2):
‚îÇ  ‚îú‚îÄ Best 1: gap=[3.0, 4.0, 5.1]
‚îÇ  ‚îî‚îÄ Best 2: gap=[4.0, 5.1, 5.2]
‚îÇ
‚îú‚îÄ Mean & Context:
‚îÇ  ‚îú‚îÄ fill_value = [(3.0+4.0)/2, (4.0+5.1)/2, (5.1+5.2)/2]
‚îÇ  ‚îÇ            = [3.5, 4.55, 5.15]
‚îÇ  ‚îî‚îÄ context_feature = concat(left, right, mean, std)
‚îÇ
‚îî‚îÄ Fill:
   ‚îú‚îÄ imputed_full = [1.0, 2.0, 3.5, 4.55, 5.15, 5.0, 6.0]
   ‚îî‚îÄ Store context for DL input

OUTPUT:
‚îú‚îÄ imputed_full: [1.0, 2.0, 3.5, 4.55, 5.15, 5.0, 6.0]
‚îú‚îÄ context_vectors: {(2,4): context_feature}
‚îî‚îÄ Ready for Deep Learning!
```

---

# D. 11 MODELS MACHINE LEARNING - CHI TI·∫æT C·∫∂N K·∫º

## D.1. WBDI (Weighted Bi-Directional Imputation) Overview

### D.1.1. Kh√°i Ni·ªám Ch√≠nh

```
WBDI = Weighted Bi-Directional Imputation

√ù t∆∞·ªüng:
‚îú‚îÄ Forward pass: S·ª≠ d·ª•ng data T·ª™ TR∆Ø·ªöC (qu√° kh·ª©)
‚îú‚îÄ Backward pass: S·ª≠ d·ª•ng data T·ª™ SAU (t∆∞∆°ng lai)
‚îî‚îÄ Combine: Weighted average c·ªßa forward & backward

SLIDING WINDOW APPROACH:
‚îú‚îÄ Window size: 8 gi·ªù (1/3 ng√†y)
‚îú‚îÄ For each missing value:
‚îÇ  ‚îú‚îÄ Forward: Predict d√πng 8 gi·ªù tr∆∞·ªõc
‚îÇ  ‚îú‚îÄ Backward: Predict d√πng 8 gi·ªù sau
‚îÇ  ‚îî‚îÄ Final: (Forward + Backward) / 2
‚îÇ
‚îî‚îÄ Advantage: S·ª≠ d·ª•ng context t·ª´ c·∫£ 2 h∆∞·ªõng
```

### D.1.2. Preprocessing cho ML Models

```python
# SLIDING WINDOW CREATION:
window_size = 8

X = []  # Input features: 8 gi·ªù Waterlevel
y = []  # Target: Average value

for i in range(window_size, len(data)):
    # Get previous 8 hours of Waterlevel
    X.append(waterlevel[i-window_size:i])
    # Target at this position
    y.append(average[i])

X = np.array(X)  # Shape: (N, 8)
y = np.array(y)  # Shape: (N,)

# Filter: Keep only rows where average is NOT NaN
mask = ~np.isnan(y)
X_clean = X[mask]
y_clean = y[mask]

# Train set creation
N = len(X_clean)
train_size = int(0.8 * N)

X_train = X_clean[:train_size]
y_train = y_clean[:train_size]
X_test = X_clean[train_size:]
y_test = y_clean[train_size:]

print(f"Training set: {len(X_train)} samples")
print(f"Testing set: {len(X_test)} samples")
print(f"Feature dimension: {X_train.shape[1]}")  # 8
```

## D.2. 11 Models - M·ªói Model Chi Ti·∫øt

### D.2.1. Model 1: Linear Regression

```python
from sklearn.linear_model import LinearRegression

# THEORY:
# Fit a hyperplane: y = w1*x1 + w2*x2 + ... + w8*x8 + b
# Minimize: loss = sum((y_true - y_pred)^2)
# Solution: Normal equation or Gradient Descent

model = LinearRegression()
model.fit(X_train, y_train)

# COEFFICIENTS:
print(f"Coefficients: {model.coef_}")  # [w1, w2, ..., w8]
print(f"Intercept: {model.intercept_}")  # b

# PREDICTION:
y_pred = model.predict(X_test)

# EXAMPLE COEFFICIENTS:
# [0.1, 0.15, 0.2, 0.25, 0.15, 0.1, 0.05, 0.01]
# Meaning: Recent hours (x7, x8) have more weight
#          Older hours (x1) have less weight

# METRICS:
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

# RESULT:
# MAE: 0.7785
# RMSE: 1.0898
# ‚úì Good baseline for simple linear relationship
```

### D.2.2. Model 2: K-Nearest Neighbors (KNN)

```python
from sklearn.neighbors import KNeighborsRegressor

# THEORY:
# For each test sample:
# 1. Find K nearest training samples (by distance)
# 2. Average their y values
# 3. That's the prediction

model = KNeighborsRegressor(n_neighbors=5)
model.fit(X_train, y_train)

# DISTANCE CALCULATION:
# Distance = sqrt(sum((X_test[i] - X_train[j])^2))
# For each test sample, find 5 nearest neighbors

# PREDICTION EXAMPLE:
# Test sample: [100, 105, 110, 115, 110, 100, 95, 90]
# 5 nearest neighbors in training set:
#   1. [101, 104, 109, 114, 111, 99, 96, 89] ‚Üí y=120
#   2. [99, 106, 111, 116, 109, 101, 94, 91] ‚Üí y=121
#   3. [102, 105, 108, 113, 112, 98, 97, 88] ‚Üí y=119
#   4. [100, 107, 110, 115, 110, 100, 95, 90] ‚Üí y=122
#   5. [98, 104, 111, 114, 108, 99, 96, 91] ‚Üí y=120
# Prediction: mean([120, 121, 119, 122, 120]) = 120.4

y_pred = model.predict(X_test)

# METRICS:
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

# RESULT:
# MAE: 1.5146
# RMSE: 2.1837
# ‚ö†Ô∏è Worse than Linear Regression
# Reason: KNN sensitive to outliers, window size small
```

### D.2.3. Model 3: Support Vector Machine (SVM)

```python
from sklearn.svm import SVR

# THEORY:
# Find hyperplane that maximizes margin while fitting data
# Use kernel trick for non-linear boundaries
# Regularization parameter C controls overfitting

model = SVR(kernel='rbf', C=100, epsilon=0.1)
model.fit(X_train, y_train)

# KERNEL: RBF (Radial Basis Function)
# K(x, x') = exp(-gamma * ||x - x'||^2)
# Captures non-linear patterns

y_pred = model.predict(X_test)

# METRICS:
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

# RESULT:
# MAE: 0.7946
# RMSE: 2.2987
# ~ Similar to Linear Regression
```

### D.2.4. Model 4: Decision Tree

```python
from sklearn.tree import DecisionTreeRegressor

# THEORY:
# Recursively split data based on features
# Each node: find best split that minimizes MSE
# Leaf node: predict average of samples there

model = DecisionTreeRegressor(max_depth=15, random_state=42)
model.fit(X_train, y_train)

# TREE EXAMPLE:
#           Root
#         /      \
#    If x5 < 105  If x5 >= 105
#     /    \        /    \
#   Leaf  Leaf    Leaf   Leaf
# pred:  pred:   pred:  pred:
#  118    122     125    128

y_pred = model.predict(X_test)

# METRICS:
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

# RESULT:
# MAE: 0.0604
# RMSE: 0.2681
# ‚úì‚úì EXCELLENT! R·∫•t t·ªët
# Reason: Decision tree perfect fit on training data
# ‚ö†Ô∏è Warning: Likely overfitting!
```

### D.2.5. Model 5: Bagging Regressor

```python
from sklearn.ensemble import BaggingRegressor

# THEORY:
# 1. Create multiple subsets of training data (with replacement)
# 2. Train separate model on each subset
# 3. Average predictions (reduce variance)

model = BaggingRegressor(n_estimators=50, random_state=42)
model.fit(X_train, y_train)

# PROCESS:
# Subset 1: Train model 1
# Subset 2: Train model 2
# ...
# Subset 50: Train model 50
# Final prediction: mean([pred1, pred2, ..., pred50])

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.6730
# RMSE: 1.0454
```

### D.2.6. Model 6: Random Forest

```python
from sklearn.ensemble import RandomForestRegressor

# THEORY:
# 1. Build many random decision trees
# 2. At each split, use random subset of features
# 3. Average predictions from all trees
# 4. Reduces overfitting compared to single tree

model = RandomForestRegressor(n_estimators=100, max_depth=15, 
                              random_state=42)
model.fit(X_train, y_train)

# FOREST WITH 100 TREES:
# Tree 1: Uses features [x1, x3, x5, x7]
# Tree 2: Uses features [x2, x4, x6, x8]
# ...
# Tree 100: Uses features [x1, x2, x3, x8]
# Final: mean(all predictions)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.6419
# RMSE: 0.9820
# ‚úì Good balance between bias & variance
```

### D.2.7. Model 7: Extra Trees (Extremely Randomized Trees)

```python
from sklearn.ensemble import ExtraTreesRegressor

# THEORY:
# Similar to Random Forest but:
# 1. Random split thresholds at each feature
# 2. Fewer computations (faster)
# 3. Different bias-variance tradeoff

model = ExtraTreesRegressor(n_estimators=100, max_depth=15,
                            random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.0912
# RMSE: 0.1374
# ‚úì‚úì BEST among ML models!
# Very similar to Decision Tree (both tree-based)
```

### D.2.8. Model 8: AdaBoost Regressor

```python
from sklearn.ensemble import AdaBoostRegressor

# THEORY:
# 1. Train weak learner (small tree)
# 2. Increase weight on misclassified samples
# 3. Train next model on weighted data
# 4. Repeat, then combine all models

model = AdaBoostRegressor(n_estimators=100, learning_rate=0.5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 8.1616
# RMSE: 10.4113
# ‚ùå WORST! Very bad
# Reason: Not suitable for this regression task
#         Better for classification
```

### D.2.9. Model 9: Gradient Boosting

```python
from sklearn.ensemble import GradientBoostingRegressor

# THEORY:
# 1. Train first weak learner
# 2. Compute residuals (y_true - y_pred)
# 3. Train next learner on residuals
# 4. Update predictions: new_pred = old_pred + learning_rate * residual_pred
# 5. Repeat

model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
                                  max_depth=7)
model.fit(X_train, y_train)

# PROCESS:
# Iteration 1: Tree 1 predicts [120, 121, 119, ...]
# Residuals: y_true - pred = [5, -2, 3, ...]
# Iteration 2: Tree 2 predicts residuals
# Iteration 3-100: Continue...

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.4808
# RMSE: 0.6051
# ‚úì Very good! Balanced performance
```

### D.2.10. Model 10: XGBoost

```python
import xgboost as xgb

# THEORY:
# Extreme Gradient Boosting
# Similar to Gradient Boosting but:
# 1. More optimized (faster training)
# 2. Regularization (L1, L2)
# 3. Better handling of missing values
# 4. Tree pruning

model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1,
                         max_depth=7)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.9622
# RMSE: 1.5714
# ‚úì Reasonable, but not as good as Gradient Boosting
```

### D.2.11. Model 11: Voting Regressor

```python
from sklearn.ensemble import VotingRegressor

# THEORY:
# 1. Train multiple diverse models
# 2. Average their predictions
# 3. Combines strengths of all models

# Base models
lr = LinearRegression()
rf = RandomForestRegressor(n_estimators=100, max_depth=15)
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)
et = ExtraTreesRegressor(n_estimators=100, max_depth=15)

# Voting ensemble
model = VotingRegressor(
    estimators=[
        ('lr', lr),
        ('rf', rf),
        ('gb', gb),
        ('et', et)
    ]
)
model.fit(X_train, y_train)

# PREDICTION PROCESS:
# pred_lr = LinearRegression.predict(X_test) = [120, 121, 119, ...]
# pred_rf = RandomForest.predict(X_test) = [121, 120, 120, ...]
# pred_gb = GradientBoosting.predict(X_test) = [119, 122, 118, ...]
# pred_et = ExtraTrees.predict(X_test) = [120.5, 120.5, 119.5, ...]
# Final: mean([120, 121, 119.5, 120]) = 120.125

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# RESULT:
# MAE: 0.7581
# RMSE: 1.0009
# ‚úì Good, combines strengths
```

## D.3. Comparative Performance - 11 Models

```
RANKING BY MAE (Lower is Better):
1. Decision Tree:        0.0604  ‚úì‚úì‚úì
2. Extra Trees:          0.0912  ‚úì‚úì‚úì
3. Gradient Boosting:    0.4808  ‚úì‚úì
4. Bagging:              0.6730  ‚úì
5. Voting Regressor:     0.7581  ‚úì
6. Linear Regression:    0.7785  ‚úì
7. SVM:                  0.7946  ‚úì
8. XGBoost:              0.9622  ‚úì
9. Random Forest:        0.6419  ‚úì
10. KNN:                 1.5146  ~
11. AdaBoost:            8.1616  ‚ùå

RANKING BY RMSE (Lower is Better):
1. Decision Tree:        0.2681  ‚úì‚úì‚úì
2. Extra Trees:          0.1374  ‚úì‚úì‚úì
3. Gradient Boosting:    0.6051  ‚úì‚úì
... (similar order)

KEY FINDINGS:
‚îú‚îÄ Tree-based models (Decision Tree, Extra Trees, GB): Best
‚îú‚îÄ Ensemble methods: Good
‚îú‚îÄ Linear/SVM: Acceptable
‚îú‚îÄ AdaBoost: Not suitable
‚îî‚îÄ Decision Tree & Extra Trees: ~0.06 MAE (maybe overfitting?)
```

---

# E. DEEP LEARNING ARCHITECTURE - ULTRA CHI TI·∫æT

## E.1. 3 Input Channels - C·ª• Th·ªÉ Chi Ti·∫øt

### E.1.1. Channel 1: Sequence Input

```
NAME: seq_in
SHAPE: (batch_size, 20)
DTYPE: float32

CONTENT:
‚îú‚îÄ Last 20 values of NORMALIZED imputed sequence
‚îú‚îÄ Normalization: MinMaxScaler([0, 1])
‚îî‚îÄ Purpose: Provide raw temporal pattern

EXAMPLE DATA (for 1 sample):
seq_in = [0.12, 0.15, 0.18, 0.20, 0.22, 0.25, ..., 0.35]
                                       ‚îî‚îÄ 20 values
         (normalized 0-1 range)

WHAT THE MODEL LEARNS FROM THIS:
‚îú‚îÄ Trend: Increasing from 0.12 to 0.35
‚îú‚îÄ Acceleration: Slope changes over time
‚îú‚îÄ Patterns: Subtle variations (seasonality)
‚îî‚îÄ Direct signal: The sequence itself
```

### E.1.2. Channel 2: Reference Input

```
NAME: ref_in
SHAPE: (batch_size, 20)
DTYPE: float32

CONTENT:
‚îú‚îÄ Last 20 values of NORMALIZED reference (Waterlevel)
‚îú‚îÄ Normalization: MinMaxScaler([0, 1])
‚îî‚îÄ Purpose: Provide comparison/context

EXAMPLE DATA:
ref_in = [0.11, 0.14, 0.17, 0.19, 0.23, 0.24, ..., 0.34]
         (same timeframe as seq_in)

COMPARISON:
seq_in vs ref_in:
‚îú‚îÄ seq_in: [0.12, 0.15, 0.18, 0.20, 0.22, 0.25, ..., 0.35]
‚îú‚îÄ ref_in: [0.11, 0.14, 0.17, 0.19, 0.23, 0.24, ..., 0.34]
‚îî‚îÄ Difference: Typically small (both from same source/period)

WHAT THE MODEL LEARNS:
‚îú‚îÄ Comparison: How different imputed is from reference
‚îú‚îÄ Error signal: Helps correct imputation bias
‚îú‚îÄ Alignment: Whether trends match
‚îî‚îÄ Validation: Signal about confidence
```

### E.1.3. Channel 3: Context Input

```
NAME: ctx_in
SHAPE: (batch_size, context_dim)
       where context_dim = 8-12 (depends on window)
DTYPE: float32

CONTENT (from eDTWBI extraction):
‚îî‚îÄ context_feature = [left_1, left_2, left_3,
                      right_1, right_2, right_3,
                      mean_gap, std_gap]

EXAMPLE DATA (context_dim=8):
ctx_in = [0.10, 0.12, 0.15,     # left context (3 values)
          0.30, 0.32, 0.35,     # right context (3 values)
          0.22,                  # mean of best gaps
          0.08]                  # std of best gaps

MEANING OF EACH COMPONENT:
1. Left context [0.10, 0.12, 0.15]:
   ‚îî‚îÄ Behavior BEFORE the gap (normalized)
   
2. Right context [0.30, 0.32, 0.35]:
   ‚îî‚îÄ Behavior AFTER the gap (normalized)
   
3. Mean of best gaps [0.22]:
   ‚îî‚îÄ Average value of best-matching gap candidates
   ‚îî‚îÄ Indicates: What values are typical for this gap?
   
4. Std of best gaps [0.08]:
   ‚îî‚îÄ Standard deviation of top-k candidates
   ‚îî‚îÄ Indicates: How consistent are the candidates?
   ‚îî‚îÄ Low std: Confident imputation
   ‚îî‚îÄ High std: Uncertain imputation

WHAT THE MODEL LEARNS:
‚îú‚îÄ Pattern quality: From mean & std
‚îú‚îÄ Confidence: Low std = high confidence
‚îú‚îÄ Context: Surrounding behavior patterns
‚îú‚îÄ Prior: From eDTWBI (strong signal)
‚îî‚îÄ Decision: Refine or trust eDTWBI
```

## E.2. Reshape & Concatenate Operations

```python
# INPUT SHAPES BEFORE RESHAPE:
seq_in shape:     (batch, 20)        # 1D array
ref_in shape:     (batch, 20)        # 1D array

# RESHAPE OPERATION:
seq_r = Reshape((20, 1))(seq_in)
ref_r = Reshape((20, 1))(ref_in)

# SHAPES AFTER RESHAPE:
seq_r shape:      (batch, 20, 1)     # 2D: 20 timesteps, 1 feature
ref_r shape:      (batch, 20, 1)     # 2D: 20 timesteps, 1 feature

# WHY RESHAPE?
‚îú‚îÄ LSTM/GRU expects 3D input: (batch, timesteps, features)
‚îú‚îÄ seq_in (batch, 20) is 2D
‚îú‚îÄ Reshape to (batch, 20, 1) makes it 3D
‚îî‚îÄ 1 feature per timestep (the sequence value)

# CONCATENATE OPERATION:
merged_seq = Concatenate(axis=-1)([seq_r, ref_r])

# CONCATENATION DETAILS:
seq_r shape:        (batch, 20, 1)
ref_r shape:        (batch, 20, 1)
axis=-1 means:      Concatenate along last axis (features)

# RESULT:
merged_seq shape:   (batch, 20, 2)
                    ‚îî‚îÄ 20 timesteps, 2 features per timestep

# FOR EACH TIMESTEP t:
merged_seq[:, t, :] = [seq_r[:, t, 0], ref_r[:, t, 0]]
                    = [sequence value, reference value]

# EXAMPLE:
seq_r[:, 0, :] = [0.12]
ref_r[:, 0, :] = [0.11]
merged_seq[:, 0, :] = [0.12, 0.11]

# VISUAL:
Before:  seq_r        ref_r        After:  merged_seq
timestep  value       value       timestep  (seq, ref)
0         0.12        0.11    ‚Üí   0         (0.12, 0.11)
1         0.15        0.14    ‚Üí   1         (0.15, 0.14)
2         0.18        0.17    ‚Üí   2         (0.18, 0.17)
...       ...         ...    ‚Üí   ...       ...
19        0.35        0.34    ‚Üí   19        (0.35, 0.34)
```

## E.3. Bidirectional LSTM - Chi Ti·∫øt Ho√†n Ch·ªânh

### E.3.1. Standard LSTM vs Bidirectional LSTM

```
STANDARD LSTM (UNIDIRECTIONAL):
    Input: (batch, 20, 2)
    ‚Üì
    [LSTM layer: forward only]
    ‚Üì
    Output: (batch, 20, 32) if return_sequences=True
            (batch, 32) if return_sequences=False
    
    Processing: t=0 ‚Üí t=1 ‚Üí t=2 ‚Üí ... ‚Üí t=19
    ‚îî‚îÄ Only uses past information (left-to-right)

BIDIRECTIONAL LSTM:
    Input: (batch, 20, 2)
    ‚Üô ‚Üò
Forward LSTM    Backward LSTM
    ‚Üì               ‚Üì
    (batch, 20, 32)  (batch, 20, 32)
    ‚Üì               ‚Üì
    t=0‚Üí19          t=19‚Üí0 (reverse)
    ‚Üô ‚Üò
    Concatenate
    ‚Üì
    Output: (batch, 20, 64) if return_sequences=True
            (batch, 64) if return_sequences=False
            
    ‚îî‚îÄ Uses both past AND future information!
```

### E.3.2. Forward & Backward LSTM Processing

```
FORWARD LSTM (Left to Right):
    t=0: h_fwd[0] = LSTM(x[0], h_fwd[-1])
    t=1: h_fwd[1] = LSTM(x[1], h_fwd[0])
    t=2: h_fwd[2] = LSTM(x[2], h_fwd[1])
    ...
    t=19: h_fwd[19] = LSTM(x[19], h_fwd[18])
    
    Output: h_fwd[0], h_fwd[1], ..., h_fwd[19]
    ‚îî‚îÄ Each uses information from t=0 to t
    ‚îî‚îÄ h_fwd[19] contains full sequence info (best)

BACKWARD LSTM (Right to Left):
    t=19: h_bwd[19] = LSTM(x[19], h_bwd[20])
    t=18: h_bwd[18] = LSTM(x[18], h_bwd[19])
    t=17: h_bwd[17] = LSTM(x[17], h_bwd[18])
    ...
    t=0: h_bwd[0] = LSTM(x[0], h_bwd[1])
    
    Output (in order): h_bwd[0], h_bwd[1], ..., h_bwd[19]
    ‚îî‚îÄ Each uses information from t=19 to t
    ‚îî‚îÄ h_bwd[0] contains full sequence info (best)

CONCATENATION AT EACH TIMESTEP:
    At t=0: concat([h_fwd[0], h_bwd[0]])
            = concat([future info from 0‚Üí1‚Üí...‚Üí19,
                     future info from 0‚Üê1‚Üê...‚Üê19])
            = (32 + 32 = 64 dims)

    At t=10: concat([h_fwd[10], h_bwd[10]])
             = concat([past 0-10 + future 10-19])
             = (64 dims - most informative!)

    At t=19: concat([h_fwd[19], h_bwd[19]])
             = concat([all past info, future 19])
             = (64 dims)
```

### E.3.3. LSTM Layer 1: return_sequences=True

```python
# LAYER 1:
x = Bidirectional(LSTM(units=32, return_sequences=True))(merged_seq)

# INPUT:
merged_seq shape: (batch, 20, 2)
                  ‚îî‚îÄ 20 timesteps, 2 features

# LSTM PARAMETERS:
units = 32
‚îî‚îÄ Each LSTM cell outputs 32-dim vector

# return_sequences=True
‚îî‚îÄ Return output at EVERY timestep
‚îî‚îÄ NOT just the last output

# OUTPUT SHAPE:
x shape: (batch, 20, 64)
         ‚îî‚îÄ 20 timesteps (same as input!)
         ‚îî‚îÄ 64 features = 32 forward + 32 backward

# OUTPUT CONTENT:
x[0] = concat(forward_LSTM[0], backward_LSTM[0]) = 64-dim vector
x[1] = concat(forward_LSTM[1], backward_LSTM[1]) = 64-dim vector
...
x[19] = concat(forward_LSTM[19], backward_LSTM[19]) = 64-dim vector

# WHY return_sequences=True?
‚îú‚îÄ We want output at each timestep
‚îú‚îÄ Later, we apply another LSTM on top
‚îú‚îÄ So we need (batch, 20, 64) not (batch, 64)
‚îî‚îÄ The second LSTM processes all 20 timesteps
```

### E.3.4. Dropout(0.2)

```python
x = Dropout(0.2)(x)

# WHAT DROPOUT DOES:
‚îú‚îÄ Randomly set 20% of values to 0 during training
‚îú‚îÄ Scale remaining 80% by 1/0.8 = 1.25
‚îî‚îÄ During inference: NO dropout, use all weights

# EFFECT:
Before dropout: x shape (batch, 20, 64)
                values: [0.5, -0.3, 0.8, ..., -0.2]

After dropout (training):
                values: [0.5*1.25, 0, 0.8*1.25, ..., 0]
                        ‚îî‚îÄ ~20% zeroed

During inference (test):
                No dropout applied
                Use full x

# WHY DROPOUT?
‚îú‚îÄ Prevent overfitting
‚îú‚îÄ Act as regularization (ensemble-like)
‚îú‚îÄ Reduce co-adaptation of neurons
‚îî‚îÄ Improve generalization
```

### E.3.5. LSTM Layer 2: return_sequences=False

```python
x = Bidirectional(LSTM(units=32))(x)

# INPUT:
x shape: (batch, 20, 64)
         ‚îî‚îÄ 20 timesteps, 64 features

# LSTM PROCESSING:
Forward LSTM:
  t=0: h_f[0] = LSTM(x[0, :], h_f[-1])  ‚Üê input: 64-dim
  t=1: h_f[1] = LSTM(x[1, :], h_f[0])
  ...
  t=19: h_f[19] = LSTM(x[19, :], h_f[18])  ‚Üê final hidden state

Backward LSTM:
  t=19: h_b[19] = LSTM(x[19, :], h_b[20])
  ...
  t=0: h_b[0] = LSTM(x[0, :], h_b[1])  ‚Üê final hidden state

# RETURN (return_sequences=False):
Only return LAST output (concatenated):
x = concat([h_f[19], h_b[0]])
  = concat([forward final, backward final])
  = 64-dim vector

# OUTPUT SHAPE:
x shape: (batch, 64)
         ‚îî‚îÄ 64 = 32 forward + 32 backward
         ‚îî‚îÄ Collapsed from (batch, 20, 64) to (batch, 64)

# WHY return_sequences=False?
‚îú‚îÄ We only care about final representation
‚îú‚îÄ This representation should encode the full sequence
‚îú‚îÄ Simplifies downstream processing
‚îî‚îÄ h_f[19] has info from entire forward pass
‚îî‚îÄ h_b[0] has info from entire backward pass
```

## E.4. Context Branch Processing

```python
# INPUT:
ctx_in shape: (batch, context_dim=12)
              ‚îî‚îÄ 12-dim vector from eDTWBI

# LAYER 1: Dense
ctx_branch = Dense(32, activation='relu')(ctx_in)

# Dense layer computation:
ctx_branch = relu(ctx_in @ W + b)
where:
  W: (12, 32) weight matrix
  b: (32,) bias vector
  @: matrix multiplication

# OUTPUT:
ctx_branch shape: (batch, 32)
                 ‚îî‚îÄ Project from 12 ‚Üí 32 dims

# ACTIVATION (ReLU):
relu(x) = max(0, x)
‚îî‚îÄ Non-linearity, captures complex patterns

# LAYER 2: Dropout
ctx_branch = Dropout(0.2)(ctx_branch)

# OUTPUT:
ctx_branch shape: (batch, 32)
                 ‚îî‚îÄ (same shape, but ~20% zeroed in training)
```

## E.5. Fusion & Final Layers

```python
# CONCATENATE SEQ BRANCH + CONTEXT BRANCH:
concat = Concatenate()([x, ctx_branch])

# BEFORE CONCATENATE:
x shape:         (batch, 64)  ‚Üê from BiLSTM
ctx_branch shape: (batch, 32)  ‚Üê from Dense + Dropout

# AFTER CONCATENATE:
concat shape: (batch, 96)
            = (batch, 64 + 32)

# LAYER 1: Dense + Dropout
z = Dense(32, activation='relu')(concat)
# (batch, 96) ‚Üí (batch, 32)

z = Dropout(0.1)(z)
# Dropout 10% (lower than before, final stage)
# (batch, 32) ‚Üí (batch, 32)

# LAYER 2: Output Dense
out = Dense(1)(z)  # No activation (Linear)
# (batch, 32) ‚Üí (batch, 1)

# FINAL OUTPUT:
out shape: (batch, 1)
out values: [0.34, 0.28, 0.41, ...]
‚îî‚îÄ Normalized predictions [0, 1]
‚îî‚îÄ Each value is imputed normalized value
```

---

# F. TRAINING & OPTIMIZATION - CHI TI·∫æT C·∫∂N K·∫º

## F.1. Callbacks Configuration

### F.1.1. EarlyStopping Chi Ti·∫øt

```python
callbacks.append(
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    )
)

# PARAMETER MEANINGS:

1. monitor='val_loss'
   ‚îú‚îÄ Watch validation loss
   ‚îú‚îÄ Epoch result should be: val_loss = 0.0523
   ‚îî‚îÄ If this metric doesn't improve, consider stopping

2. patience=10
   ‚îú‚îÄ Wait 10 epochs without improvement
   ‚îú‚îÄ Example:
   ‚îÇ  Epoch 15: val_loss = 0.0620 (BEST so far)
   ‚îÇ  Epoch 16: val_loss = 0.0625 (no improve) ‚Üí count=1
   ‚îÇ  Epoch 17: val_loss = 0.0628 (no improve) ‚Üí count=2
   ‚îÇ  ...
   ‚îÇ  Epoch 25: val_loss = 0.0750 (no improve) ‚Üí count=10
   ‚îÇ  Epoch 26: STOP! (count reached patience)
   ‚îî‚îÄ Restore weights from Epoch 15 (best val_loss)

3. restore_best_weights=True
   ‚îú‚îÄ After stopping, load the best weights
   ‚îú‚îÄ Not the final weights (which might be worse)
   ‚îî‚îÄ Use model from Epoch 15 (lowest val_loss)

4. verbose=1
   ‚îú‚îÄ Print messages
   ‚îú‚îÄ Output:
   ‚îÇ  Epoch 26/50: EarlyStopping: Stop training
   ‚îÇ              Restoring model weights from the epoch
   ‚îÇ              with the best validation loss
   ‚îî‚îÄ Helps track what's happening
```

### F.1.2. ReduceLROnPlateau Chi Ti·∫øt

```python
callbacks.append(
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        verbose=1
    )
)

# PARAMETER MEANINGS:

1. monitor='val_loss'
   ‚îî‚îÄ Watch validation loss

2. factor=0.5
   ‚îú‚îÄ Multiply learning rate by 0.5
   ‚îú‚îÄ If LR was 0.001, new LR = 0.001 * 0.5 = 0.0005
   ‚îî‚îÄ Reduce learning rate to half

3. patience=3
   ‚îú‚îÄ Wait 3 epochs without improvement before reducing LR
   ‚îî‚îÄ After 3 epochs of no progress, reduce LR

4. EXAMPLE EXECUTION:

Epoch 1-5: LR = 0.001
  - Epoch 1: val_loss = 0.2500 (BEST)
  - Epoch 2: val_loss = 0.1800 (BEST)
  - Epoch 3: val_loss = 0.1200 (BEST)
  - Epoch 4: val_loss = 0.0800 (BEST)
  - Epoch 5: val_loss = 0.0600 (BEST)

Epoch 6-8: Still LR = 0.001, but no improvement
  - Epoch 6: val_loss = 0.0620 (no improve) ‚Üí count=1
  - Epoch 7: val_loss = 0.0630 (no improve) ‚Üí count=2
  - Epoch 8: val_loss = 0.0635 (no improve) ‚Üí count=3 ‚Üí count=patience!

Epoch 9 onwards: LR REDUCED to 0.0005
  - Epoch 9: val_loss = 0.0580 (IMPROVE!) ‚úì count reset to 0
  - Epoch 10: val_loss = 0.0550 (IMPROVE!) ‚úì
  - Epoch 11-13: No improve, count=1,2,3

Epoch 14 onwards: LR REDUCED again to 0.00025
  - Continue training with smaller LR...

# WHY ReduceLROnPlateau?
‚îú‚îÄ When gradient becomes small, updates slow
‚îú‚îÄ Smaller LR = finer adjustments
‚îú‚îÄ Helps escape local plateaus
‚îî‚îÄ Improves convergence
```

## F.2. Training Loop

```python
model = build_model('LSTM', sequence_length=20, context_dim=12, units=32)

history = model.fit(
    [X_seq_tr, X_ref_tr, X_ctx_tr],  # Inputs
    y_tr,                             # Target
    validation_split=0.15,            # 15% of training for validation
    epochs=50,                        # Maximum 50 epochs
    batch_size=128,                   # 128 samples per batch
    callbacks=callbacks,              # EarlyStopping + ReduceLROnPlateau
    verbose=2                         # Detailed output
)

# TRAINING PROCESS:

Epoch 1/50
----------
Batch 1/176: loss = 0.2541, mae = 0.4521
Batch 2/176: loss = 0.2304, mae = 0.4200
...
Batch 176/176: loss = 0.1800, mae = 0.3900

Training loss:   0.1950
Validation loss: 0.1823 (BEST!)
Learning rate:   0.001000

Epoch 2/50
----------
Training loss:   0.1234
Validation loss: 0.1156 (BEST!)
Learning rate:   0.001000

...

Epoch 15/50
----------
Training loss:   0.0023
Validation loss: 0.0052 (BEST!)
Learning rate:   0.001000

Epoch 16-18/50
----------
Validation loss increases (no improvement)
LR reduced to 0.0005

Epoch 25/50
----------
Training loss:   0.0005
Validation loss: 0.0048 (no improve vs Epoch 15)
count = 10 (patience reached!)

EarlyStopping: Restoring best model weights from Epoch 15
Training stopped!

# HISTORY OBJECT:
history.history = {
    'loss': [0.195, 0.123, 0.089, ..., 0.0005],
    'mae': [0.384, 0.301, 0.224, ..., 0.0041],
    'val_loss': [0.1823, 0.1156, 0.0898, ..., 0.0048],
    'val_mae': [0.3956, 0.2987, 0.2234, ..., 0.0045]
}
```

## F.3. Mixed Precision Training

```python
from tensorflow.keras import mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# WHAT IS MIXED PRECISION?
‚îú‚îÄ Forward pass: Use float16 (16-bit floating point)
‚îÇ  ‚îú‚îÄ Faster computation
‚îÇ  ‚îú‚îÄ Less memory usage
‚îÇ  ‚îî‚îÄ Less accurate but usually fine
‚îÇ
‚îî‚îÄ Loss computation: Use float32 (32-bit floating point)
   ‚îú‚îÄ Better precision for gradients
   ‚îú‚îÄ Avoids numerical instability
   ‚îî‚îÄ Slow but critical

# PERFORMANCE GAINS:
‚îú‚îÄ Speed: 1.5-2x faster on modern GPUs
‚îú‚îÄ Memory: 50% reduction in memory usage
‚îú‚îÄ Accuracy: Usually same as float32
‚îî‚îÄ Suitable for: Batch size, learning, general training

# TRADEOFFS:
‚îú‚îÄ ‚úì Faster, uses less memory
‚îú‚îÄ ‚ùå Potential numerical issues (rare with modern GPUs)
‚îî‚îÄ ‚Üí Use for Deep Learning, not for critical financial calculations
```

---

# G. METRICS & EVALUATION - C·∫∂N K·∫º

(Ti·∫øp t·ª•c v·ªõi ph·∫ßn H-K...)

---

**T√†i li·ªáu n√†y qu√° d√†i (>10,000 t·ª´), t√¥i s·∫Ω t·∫°o m·ªôt file m·ªõi cho ph·∫ßn c√≤n l·∫°i...**
